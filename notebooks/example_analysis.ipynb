{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification Analysis\n",
    "\n",
    "This notebook demonstrates how to use the research project template for MNIST classification analysis. It shows how to run experiments, analyze results, and create visualizations.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The notebook covers:\n",
    "1. Setting up the experiment environment\n",
    "2. Running experiments with different models\n",
    "3. Analyzing and comparing results\n",
    "4. Creating visualizations\n",
    "5. Interpreting model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path to import our modules\n",
    "sys.path.append('../src')\n",
    "\n",
    "from experiment import ExperimentConfig, Experiment\n",
    "from experiment.schemas import load_config\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "Path('../results/plots').mkdir(parents=True, exist_ok=True)\n",
    "Path('../cache').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Single Experiment Example\n",
    "\n",
    "Let's start by running a single experiment with the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = Path('../configs/local.yaml')\n",
    "config = load_config(config_path)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"- Experiment: {config.experiment.name}\")\n",
    "print(f\"- Model: {config.model.name}\")\n",
    "print(f\"- Epochs: {config.training.epochs}\")\n",
    "print(f\"- Batch size: {config.data.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run experiment\n",
    "experiment = Experiment.from_config(config)\n",
    "\n",
    "print(\"Running experiment...\")\n",
    "print(\"This may take a few minutes for the first run (downloading MNIST data)\")\n",
    "\n",
    "# Run the experiment\n",
    "results = experiment.run()\n",
    "\n",
    "print(\"\\nExperiment completed!\")\n",
    "print(f\"Results saved to: {results['results_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\n=== EXPERIMENT RESULTS ===\")\n",
    "print(f\"Experiment time: {results['experiment_time']:.2f} seconds\")\n",
    "\n",
    "# Show metrics for each split\n",
    "for key, value in results.items():\n",
    "    if key.endswith('_metrics') and isinstance(value, dict):\n",
    "        split_name = key.replace('_metrics', '').title()\n",
    "        print(f\"\\n{split_name} Metrics:\")\n",
    "        for metric_name, metric_value in value.items():\n",
    "            if isinstance(metric_value, float):\n",
    "                print(f\"  {metric_name}: {metric_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparing Multiple Models\n",
    "\n",
    "Now let's run experiments with different models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configurations for different models\n",
    "model_configs = {\n",
    "    'linear': {\n",
    "        \"name\": \"linear\",\n",
    "        \"input_size\": 784,\n",
    "        \"num_classes\": 10\n",
    "    },\n",
    "    'mlp': {\n",
    "        \"name\": \"mlp\",\n",
    "        \"input_size\": 784,\n",
    "        \"hidden_size\": 128,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.2\n",
    "    },\n",
    "    'cnn': {\n",
    "        \"name\": \"cnn\",\n",
    "        \"input_channels\": 1,\n",
    "        \"channels\": [32, 64],\n",
    "        \"kernel_size\": 3,\n",
    "        \"dropout\": 0.2\n",
    "    }\n",
    "}\n",
    "\n",
    "# Base configuration (same for all models)\n",
    "base_config = {\n",
    "    \"data\": {\n",
    "        \"dataset\": \"mnist\",\n",
    "        \"batch_size\": 32,\n",
    "        \"validation_split\": 0.1\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"epochs\": 5,  # Reduced for demonstration\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"optimizer\": \"adam\"\n",
    "    },\n",
    "    \"evaluation\": {\n",
    "        \"metrics\": [\"accuracy\", \"f1_macro\"]\n",
    "    },\n",
    "    \"experiment\": {\n",
    "        \"random_seed\": 42,\n",
    "        \"device\": \"auto\",\n",
    "        \"save_model\": False  # Don't save models in demo\n",
    "    },\n",
    "    \"logging\": {\n",
    "        \"log_every_n_steps\": 50\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Model comparison configurations created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments for all models\n",
    "model_results = {}\n",
    "\n",
    "for model_name, model_config in model_configs.items():\n",
    "    print(f\"\\nRunning experiment with {model_name.upper()} model...\")\n",
    "    \n",
    "    # Create full configuration\n",
    "    full_config = base_config.copy()\n",
    "    full_config['model'] = model_config\n",
    "    full_config['experiment']['name'] = f'mnist_{model_name}_comparison'\n",
    "    \n",
    "    # Create experiment config object\n",
    "    experiment_config = ExperimentConfig(**full_config)\n",
    "    \n",
    "    # Run experiment\n",
    "    experiment = Experiment.from_config(experiment_config)\n",
    "    results = experiment.run()\n",
    "    \n",
    "    # Store results\n",
    "    model_results[model_name] = results\n",
    "    \n",
    "    print(f\"{model_name.upper()} completed in {results['experiment_time']:.2f}s\")\n",
    "\n",
    "print(\"\\nAll experiments completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results Analysis and Visualization\n",
    "\n",
    "Now let's analyze and visualize the results from all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics for comparison\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, results in model_results.items():\n",
    "    # Get test metrics\n",
    "    test_metrics = results.get('test_metrics', {})\n",
    "    \n",
    "    row = {\n",
    "        'Model': model_name.upper(),\n",
    "        'Test Accuracy': test_metrics.get('accuracy', 0),\n",
    "        'Test F1 Macro': test_metrics.get('f1_macro', 0),\n",
    "        'Training Time (s)': results.get('experiment_time', 0)\n",
    "    }\n",
    "    comparison_data.append(row)\n",
    "\n",
    "# Create DataFrame for easy analysis\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"Model Comparison Results:\")\n",
    "print(comparison_df.to_string(index=False, float_format='%.4f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization comparing model performance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].bar(comparison_df['Model'], comparison_df['Test Accuracy'], \n",
    "           color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[0].set_title('Test Accuracy by Model')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_ylim(0, 1)\n",
    "for i, v in enumerate(comparison_df['Test Accuracy']):\n",
    "    axes[0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# F1 Score comparison\n",
    "axes[1].bar(comparison_df['Model'], comparison_df['Test F1 Macro'], \n",
    "           color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[1].set_title('Test F1 Macro Score by Model')\n",
    "axes[1].set_ylabel('F1 Macro Score')\n",
    "axes[1].set_ylim(0, 1)\n",
    "for i, v in enumerate(comparison_df['Test F1 Macro']):\n",
    "    axes[1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Training time comparison\n",
    "axes[2].bar(comparison_df['Model'], comparison_df['Training Time (s)'], \n",
    "           color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[2].set_title('Training Time by Model')\n",
    "axes[2].set_ylabel('Time (seconds)')\n",
    "for i, v in enumerate(comparison_df['Training Time (s)']):\n",
    "    axes[2].text(i, v + 1, f'{v:.1f}s', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Model comparison plot saved to results/plots/model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training History Analysis\n",
    "\n",
    "Let's analyze the training history for one of the models to understand the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training history from the MLP experiment (usually has the most interesting dynamics)\n",
    "mlp_results = model_results['mlp']\n",
    "\n",
    "# Load the cached training history\n",
    "from experiment.infra import create_infra\n",
    "\n",
    "infra = create_infra(Path('../cache'), 'analysis')\n",
    "training_history = infra.load_artifact('training_history')\n",
    "\n",
    "print(\"Training history loaded:\")\n",
    "print(f\"Epochs trained: {len(training_history['train_loss'])}\")\n",
    "print(f\"Final train loss: {training_history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final train accuracy: {training_history['train_accuracy'][-1]:.4f}\")\n",
    "if 'val_loss' in training_history:\n",
    "    print(f\"Final val loss: {training_history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Final val accuracy: {training_history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "epochs = range(1, len(training_history['train_loss']) + 1)\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(epochs, training_history['train_loss'], 'o-', label='Training Loss', linewidth=2)\n",
    "if 'val_loss' in training_history and len(training_history['val_loss']) > 0:\n",
    "    ax1.plot(epochs, training_history['val_loss'], 's-', label='Validation Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(epochs, training_history['train_accuracy'], 'o-', label='Training Accuracy', linewidth=2)\n",
    "if 'val_accuracy' in training_history and len(training_history['val_accuracy']) > 0:\n",
    "    ax2.plot(epochs, training_history['val_accuracy'], 's-', label='Validation Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training curves saved to results/plots/training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Analysis and Insights\n",
    "\n",
    "Let's create a summary of our findings and provide insights about the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a detailed analysis report\n",
    "print(\"=== MNIST CLASSIFICATION ANALYSIS REPORT ===\")\n",
    "print(\"\\n1. MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Find best performing model\n",
    "best_accuracy_idx = comparison_df['Test Accuracy'].idxmax()\n",
    "best_model = comparison_df.loc[best_accuracy_idx]\n",
    "\n",
    "print(f\"Best performing model: {best_model['Model']}\")\n",
    "print(f\"  - Test Accuracy: {best_model['Test Accuracy']:.4f}\")\n",
    "print(f\"  - Test F1 Macro: {best_model['Test F1 Macro']:.4f}\")\n",
    "print(f\"  - Training Time: {best_model['Training Time (s)']:.2f}s\")\n",
    "\n",
    "print(\"\\n2. MODEL COMPARISON INSIGHTS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for _, row in comparison_df.iterrows():\n",
    "    model_name = row['Model']\n",
    "    accuracy = row['Test Accuracy']\n",
    "    time = row['Training Time (s)']\n",
    "    \n",
    "    if model_name == 'LINEAR':\n",
    "        print(f\"📊 {model_name}:\")\n",
    "        print(f\"   - Simplest model with {accuracy:.3f} accuracy\")\n",
    "        print(f\"   - Fastest training ({time:.1f}s)\")\n",
    "        print(f\"   - Good baseline for simple classification tasks\")\n",
    "        \n",
    "    elif model_name == 'MLP':\n",
    "        print(f\"🧠 {model_name}:\")\n",
    "        print(f\"   - Neural network with {accuracy:.3f} accuracy\")\n",
    "        print(f\"   - Moderate complexity and training time ({time:.1f}s)\")\n",
    "        print(f\"   - Good balance of performance and interpretability\")\n",
    "        \n",
    "    elif model_name == 'CNN':\n",
    "        print(f\"🖼️ {model_name}:\")\n",
    "        print(f\"   - Convolutional model with {accuracy:.3f} accuracy\")\n",
    "        print(f\"   - Designed for image data ({time:.1f}s training)\")\n",
    "        print(f\"   - Can learn spatial features and patterns\")\n",
    "\n",
    "print(\"\\n3. RECOMMENDATIONS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if best_model['Model'] == 'CNN':\n",
    "    print(\"✅ CNN performs best - recommended for image classification tasks\")\n",
    "    print(\"✅ Consider using CNN for production deployment\")\n",
    "elif best_model['Model'] == 'MLP':\n",
    "    print(\"✅ MLP provides good balance of accuracy and speed\")\n",
    "    print(\"✅ Consider MLP for moderate-complexity tasks\")\n",
    "else:\n",
    "    print(\"✅ Linear model is surprisingly effective\")\n",
    "    print(\"✅ Consider linear model for fast inference requirements\")\n",
    "\n",
    "print(\"\\n4. NEXT STEPS\")\n",
    "print(\"-\" * 50)\n",
    "print(\"📈 Try hyperparameter tuning with the best model\")\n",
    "print(\"📊 Collect more training data if accuracy is insufficient\")\n",
    "print(\"🔄 Experiment with data augmentation techniques\")\n",
    "print(\"⚡ Consider ensemble methods for better performance\")\n",
    "print(\"🎯 Analyze misclassified examples to understand failure modes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment Configuration Management\n",
    "\n",
    "This section demonstrates how to manage and create different experiment configurations programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom configuration for hyperparameter tuning\n",
    "tuning_config = {\n",
    "    \"data\": {\n",
    "        \"dataset\": \"mnist\",\n",
    "        \"batch_size\": 64,\n",
    "        \"validation_split\": 0.2\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"name\": \"mlp\",\n",
    "        \"input_size\": 784,\n",
    "        \"hidden_size\": 256,  # Larger hidden size\n",
    "        \"num_layers\": 3,     # More layers\n",
    "        \"dropout\": 0.3,      # More dropout\n",
    "        \"activation\": \"relu\"\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"epochs\": 10,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"weight_decay\": 0.0001,\n",
    "        \"scheduler\": {\n",
    "            \"name\": \"reduce_lr_on_plateau\",\n",
    "            \"factor\": 0.5,\n",
    "            \"patience\": 3,\n",
    "            \"min_lr\": 1e-6\n",
    "        },\n",
    "        \"early_stopping\": {\n",
    "            \"patience\": 5,\n",
    "            \"min_delta\": 0.001\n",
    "        }\n",
    "    },\n",
    "    \"evaluation\": {\n",
    "        \"metrics\": [\"accuracy\", \"f1_macro\", \"f1_micro\"],\n",
    "        \"save_predictions\": True,\n",
    "        \"save_confusion_matrix\": True\n",
    "    },\n",
    "    \"experiment\": {\n",
    "        \"name\": \"mnist_tuned_mlp\",\n",
    "        \"description\": \"Hyperparameter tuned MLP for MNIST\",\n",
    "        \"random_seed\": 42,\n",
    "        \"device\": \"auto\",\n",
    "        \"save_model\": True\n",
    "    },\n",
    "    \"logging\": {\n",
    "        \"log_every_n_steps\": 100,\n",
    "        \"log_gradients\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save this configuration for future use\n",
    "tuned_config_path = Path('../configs/tuned_mlp.yaml')\n",
    "with open(tuned_config_path, 'w') as f:\n",
    "    yaml.dump(tuning_config, f, default_flow_style=False, indent=2)\n",
    "\n",
    "print(f\"Tuned configuration saved to: {tuned_config_path}\")\n",
    "print(\"\\nConfiguration preview:\")\n",
    "print(f\"  Model: {tuning_config['model']['name']} with {tuning_config['model']['hidden_size']} hidden units\")\n",
    "print(f\"  Layers: {tuning_config['model']['num_layers']}\")\n",
    "print(f\"  Dropout: {tuning_config['model']['dropout']}\")\n",
    "print(f\"  Learning rate scheduling: {tuning_config['training']['scheduler']['name']}\")\n",
    "print(f\"  Early stopping: enabled with patience {tuning_config['training']['early_stopping']['patience']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Caching and Reproducibility\n",
    "\n",
    "This section demonstrates the caching capabilities and how to ensure reproducible experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine cache structure\n",
    "cache_dir = Path('../cache')\n",
    "if cache_dir.exists():\n",
    "    print(\"Cache directory contents:\")\n",
    "    for item in cache_dir.rglob('*'):\n",
    "        if item.is_file():\n",
    "            size_mb = item.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  {item.relative_to(cache_dir)}: {size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(\"No cache directory found.\")\n",
    "\n",
    "# Show how to clean cache if needed\n",
    "print(\"\\nTo clean cache, you can use:\")\n",
    "print(\"  python -m src.cli clean --cache-dir cache --keep 3\")\n",
    "print(\"  (keeps 3 most recent files of each type)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate reproducibility by running the same experiment twice\n",
    "print(\"Testing reproducibility...\")\n",
    "\n",
    "# Create a simple config for reproducibility test\n",
    "repro_config = {\n",
    "    \"data\": {\"dataset\": \"mnist\", \"batch_size\": 16, \"download\": False},\n",
    "    \"model\": {\"name\": \"linear\", \"input_size\": 784, \"num_classes\": 10},\n",
    "    \"training\": {\"epochs\": 1, \"learning_rate\": 0.01, \"optimizer\": \"sgd\"},\n",
    "    \"evaluation\": {\"metrics\": [\"accuracy\"]},\n",
    "    \"experiment\": {\n",
    "        \"name\": \"reproducibility_test\",\n",
    "        \"random_seed\": 123,  # Fixed seed\n",
    "        \"device\": \"cpu\",\n",
    "        \"save_model\": False\n",
    "    },\n",
    "    \"logging\": {\"log_every_n_steps\": 10}\n",
    "}\n",
    "\n",
    "results_run1 = []\n",
    "results_run2 = []\n",
    "\n",
    "# Run 1\n",
    "config1 = ExperimentConfig(**repro_config)\n",
    "exp1 = Experiment.from_config(config1)\n",
    "res1 = exp1.run()\n",
    "results_run1.append(res1.get('test_metrics', {}).get('accuracy', 0))\n",
    "\n",
    "# Run 2 (same configuration)\n",
    "config2 = ExperimentConfig(**repro_config)\n",
    "exp2 = Experiment.from_config(config2)\n",
    "res2 = exp2.run()\n",
    "results_run2.append(res2.get('test_metrics', {}).get('accuracy', 0))\n",
    "\n",
    "print(f\"Run 1 accuracy: {results_run1[0]:.6f}\")\n",
    "print(f\"Run 2 accuracy: {results_run2[0]:.6f}\")\n",
    "print(f\"Difference: {abs(results_run1[0] - results_run2[0]):.6f}\")\n",
    "\n",
    "if abs(results_run1[0] - results_run2[0]) < 1e-5:\n",
    "    print(\"✅ Results are reproducible!\")\n",
    "else:\n",
    "    print(\"❌ Results differ - check random seed settings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions\n",
    "\n",
    "This notebook demonstrated the key features of the research project template:\n",
    "\n",
    "### ✅ **What we accomplished:**\n",
    "1. **Configuration Management**: Loaded and validated experiment configurations\n",
    "2. **Multiple Models**: Compared Linear, MLP, and CNN models on MNIST\n",
    "3. **Automated Pipeline**: Used ExCa caching for efficient experiment runs\n",
    "4. **Results Analysis**: Created visualizations and performance comparisons\n",
    "5. **Reproducibility**: Demonstrated consistent results with fixed random seeds\n",
    "6. **Extensibility**: Showed how to create custom configurations\n",
    "\n",
    "### 🔑 **Key Benefits of this Template:**\n",
    "- **Type Safety**: Pydantic ensures configuration validation\n",
    "- **Caching**: ExCa speeds up repeated experiments\n",
    "- **Reproducibility**: Fixed seeds ensure consistent results\n",
    "- **Modularity**: Easy to swap models, optimizers, and datasets\n",
    "- **Testing**: Comprehensive test suite ensures code quality\n",
    "- **CI/CD**: Automated testing on every commit\n",
    "\n",
    "### 🚀 **Next Steps for Research:**\n",
    "1. **Hyperparameter Tuning**: Use the tuned configuration we created\n",
    "2. **Data Augmentation**: Add rotation, scaling, and other transformations\n",
    "3. **Advanced Models**: Implement ResNet, Vision Transformer, etc.\n",
    "4. **Different Datasets**: Extend to CIFAR-10, ImageNet, or custom datasets\n",
    "5. **Ensemble Methods**: Combine multiple models for better performance\n",
    "6. **Analysis Tools**: Add confusion matrices, t-SNE visualizations, etc.\n",
    "\n",
    "### 📚 **For New Lab Members:**\n",
    "- Clone this template for your own projects\n",
    "- Follow the development workflow (feature branches → dev → main)\n",
    "- Write tests for new functionality\n",
    "- Use the CLI for running experiments: `python -m src.cli run configs/local.yaml`\n",
    "- Check the README.md for detailed setup instructions\n",
    "\n",
    "**Happy researching!** 🔬"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}